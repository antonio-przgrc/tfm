{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.12.4\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Importación de librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Importing plotly failed. Interactive plots will not work.\n",
      "Importing plotly failed. Interactive plots will not work.\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from copy import copy as cp\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import root_mean_squared_error as rootmse\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from darts import TimeSeries\n",
    "from darts.utils.callbacks import TFMProgressBar\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "from darts.models import RNNModel, TransformerModel, Prophet, BlockRNNModel, NBEATSModel\n",
    "from darts.metrics import rmse\n",
    "from darts.utils.statistics import check_seasonality, plot_acf\n",
    "import darts.utils.timeseries_generation as tg\n",
    "from darts.utils.timeseries_generation import datetime_attribute_timeseries\n",
    "from darts.utils.missing_values import fill_missing_values\n",
    "from darts.utils.likelihood_models import GaussianLikelihood\n",
    "from darts.timeseries import concatenate\n",
    "\n",
    "from neuralprophet import NeuralProphet, utils\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def tratamiento(fichero):\n",
    "    dataframe = pd.read_excel(fichero)\n",
    "    dataframe = dataframe.rename({'clave1':'fecha', 'uniTotal':'unidades'}, axis=1)\n",
    "    dataframe['fecha'] = pd.to_datetime(dataframe['fecha'], format=\"%Y%m%d\")\n",
    "    dataframe = dataframe.set_index(['fecha'])\n",
    "    dataframe = dataframe.resample('D').first()\n",
    "    dataframe.fillna(value=0, inplace=True)\n",
    "    dataframe = dataframe.groupby(pd.Grouper(freq='B'))\n",
    "    dataframe = dataframe.sum()\n",
    "    dataframe[dataframe['unidades'] < 0] = 0\n",
    "    dataframe = dataframe[\"2010-01-01\":\"2024-06-30\"]\n",
    "    name = fichero[fichero.find('/')+1:fichero.find('.')]\n",
    "    return name, dataframe\n",
    "\n",
    "def agrupar(ficheros: list):\n",
    "    df = pd.DataFrame()\n",
    "    lista = []\n",
    "    for archivo in ficheros:\n",
    "        name, aux = tratamiento(archivo)\n",
    "        aux = aux.rename({'unidades':name}, axis=1)\n",
    "  \n",
    "      df = pd.concat([df, aux], axis=1)\n",
    "        lista.append(name)\n",
    "    return df, lista\n",
    "\n",
    "def grafico(dataframe):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(dataframe)\n",
    "    ax.minorticks_on()\n",
    "    ax.grid(which='major', alpha = 0.65, linestyle='-')\n",
    "    ax.grid(which='minor', alpha = 0.25, linestyle='--')\n",
    "    fig.autofmt_xdate()\n",
    "\n",
    "\n",
    "#df = tratamiento('data/filtros.xlsx')\n",
    "\n",
    "#df = tratamiento('data/discos.xlsx')\n",
    "\n",
    "#df = tratamiento('data/pastillas.xlsx')\n",
    "\n",
    "# name, df = tratamiento('data/baterias.xlsx') \n",
    "\n",
    "df, names = agrupar(['data/filtros.xlsx', 'data/baterias.xlsx'])\n",
    "\n",
    "df = pd.read_csv('data/meteo_olvera.csv', decimal=',')\n",
    "df['fecha'] = pd.to_datetime(df2['fecha'], format=\"%Y-%m-%d\")\n",
    "df.set_index('fecha', inplace=True)\n",
    "df = df[['tmed', 'tmin', 'tmax', 'prec', 'dir', 'velmedia', 'hrMedia']]\n",
    "df2 = df2.resample('B').first()\n",
    "df2 = df2.fillna(method='backfill')\n",
    "df = pd.concat([df, df2], axis=1)\n",
    "\n",
    "df = df.reset_index()\n",
    "\n",
    "train_df = df[:-130]\n",
    "test_df = df[-130:]\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "# scaler.fit(train_df[['unidades']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creacion de clase TimeSeries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['tmed', 'tmin', 'tmax', 'prec', 'dir', 'velmedia', 'hrMedia']\n",
    "\n",
    "train = TimeSeries.from_dataframe(train_df, time_col='fecha', value_cols=names+cols)\n",
    "test = TimeSeries.from_dataframe(test_df, time_col='fecha', value_cols=names+cols)\n",
    "\n",
    "transformer = Scaler(scaler)\n",
    "train = transformer.fit_transform(train)\n",
    "test = transformer.transform(test)\n",
    "train = train.add_holidays(country_code='ES', prov='AN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definición de modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "batch = 256\n",
    "\n",
    "mod_blockrnn = BlockRNNModel(\n",
    "    model='RNN',\n",
    "    input_chunk_length=60,\n",
    "    output_chunk_length=60,\n",
    "    hidden_dim=25,\n",
    "    n_rnn_layers=2,\n",
    "    dropout=0.1,\n",
    "    n_epochs=epochs,\n",
    "    batch_size=batch,\n",
    "    #optimizer_kwargs={\"lr\": 1e-3},\n",
    "    show_warnings=True,\n",
    "    model_name='RNN',\n",
    "    #pl_trainer_kwargs={\"precision\": '64', \"accelerator\": \"gpu\", \"devices\": -1, \"auto_select_gpus\": True} \n",
    "    pl_trainer_kwargs={\"precision\": '64', \"accelerator\": \"cpu\"} \n",
    ")\n",
    "\n",
    "mod_blockrnn_multi = cp(mod_blockrnn)\n",
    "\n",
    "mod_blocklstm = BlockRNNModel(\n",
    "    model='LSTM',\n",
    "    input_chunk_length=60,\n",
    "    output_chunk_length=60,\n",
    "    hidden_dim=25,\n",
    "    n_rnn_layers=2,\n",
    "    dropout=0.1,\n",
    "    n_epochs=epochs,\n",
    "    batch_size=batch,\n",
    "    #optimizer_kwargs={\"lr\": 1e-3},\n",
    "    show_warnings=True,\n",
    "    model_name='LSTM',\n",
    "    pl_trainer_kwargs={\"precision\": '64', \"accelerator\": \"cpu\"} \n",
    ")\n",
    "mod_blocklstm_multi = cp(mod_blocklstm)\n",
    "\n",
    "mod_blockgru = BlockRNNModel(\n",
    "    model='GRU',\n",
    "    input_chunk_length=60,\n",
    "    output_chunk_length=60,\n",
    "    hidden_dim=25,\n",
    "    n_rnn_layers=2,\n",
    "    dropout=0.1,\n",
    "    n_epochs=epochs,\n",
    "    batch_size=batch,\n",
    "    #optimizer_kwargs={\"lr\": 1e-3},\n",
    "    show_warnings=True,\n",
    "    model_name='GRU',\n",
    "    pl_trainer_kwargs={\"precision\": '64', \"accelerator\": \"cpu\"} \n",
    ")\n",
    "mod_blockgru_multi = cp(mod_blockgru)\n",
    "\n",
    "mod_prophet = Prophet(\n",
    "    add_seasonalities=None, \n",
    "    country_holidays='ES', \n",
    "    suppress_stdout_stderror=True, \n",
    "    add_encoders=None, \n",
    "    cap=None, \n",
    "    floor=None\n",
    ")\n",
    "\n",
    "mod_neuralprophet = NeuralProphet()\n",
    "\n",
    "mod_nbeats = NBEATSModel(\n",
    "    input_chunk_length=60,\n",
    "    output_chunk_length=60,\n",
    "    dropout=0.1,\n",
    "    n_epochs=epochs,\n",
    "    show_warnings=True,\n",
    "    batch_size=batch,\n",
    "    model_name='LSTM',\n",
    "    pl_trainer_kwargs={\"precision\": '64', \"accelerator\": \"cpu\"} \n",
    ")\n",
    "\n",
    "mod_nbeats_multi = cp(mod_nbeats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento o carga de modelos univariable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - (NP.forecaster.fit) - When Global modeling with local normalization, metrics are displayed in normalized scale.\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency B corresponds to 99.973% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Dataframe freq automatically defined as B\n",
      "INFO - (NP.config.init_data_params) - Setting normalization to global as only one dataframe provided for training.\n",
      "INFO - (NP.utils.set_auto_seasonalities) - Disabling daily seasonality. Run NeuralProphet with daily_seasonality=True to override this.\n",
      "INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 64\n",
      "INFO - (NP.config.set_auto_batch_epoch) - Auto-set epochs to 80\n",
      "WARNING - (NP.config.set_lr_finder_args) - Learning rate finder: The number of batches (58) is too small than the required number                     for the learning rate finder (239). The results might not be optimal.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75818a780583417ab92952eb7a78db9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/239 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd853dfc18c04caf84b01f45239d543d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - (NP.df_utils._infer_frequency) - Major frequency B corresponds to 99.231% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - B\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency B corresponds to 99.231% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - B\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ac9b7bcc1ea4a2ca3f7aa5aa5ebd987",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 58it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - (NP.df_utils.return_df_in_original_format) - Returning df with no ID column\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "      <th>yhat1</th>\n",
       "      <th>trend</th>\n",
       "      <th>season_yearly</th>\n",
       "      <th>season_weekly</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48.790421</td>\n",
       "      <td>25.645348</td>\n",
       "      <td>-3.249312</td>\n",
       "      <td>26.394384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-01-02</td>\n",
       "      <td>46.0</td>\n",
       "      <td>49.413662</td>\n",
       "      <td>25.648117</td>\n",
       "      <td>-3.039501</td>\n",
       "      <td>26.805048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-01-03</td>\n",
       "      <td>52.0</td>\n",
       "      <td>48.717308</td>\n",
       "      <td>25.650887</td>\n",
       "      <td>-2.821206</td>\n",
       "      <td>25.887630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-01-04</td>\n",
       "      <td>70.0</td>\n",
       "      <td>48.204041</td>\n",
       "      <td>25.653662</td>\n",
       "      <td>-2.595083</td>\n",
       "      <td>25.145466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-01-05</td>\n",
       "      <td>41.0</td>\n",
       "      <td>50.895260</td>\n",
       "      <td>25.656431</td>\n",
       "      <td>-2.363155</td>\n",
       "      <td>27.601984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>2024-06-24</td>\n",
       "      <td>65.0</td>\n",
       "      <td>55.712456</td>\n",
       "      <td>26.130196</td>\n",
       "      <td>3.236652</td>\n",
       "      <td>26.345610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>2024-06-25</td>\n",
       "      <td>88.0</td>\n",
       "      <td>56.166698</td>\n",
       "      <td>26.132971</td>\n",
       "      <td>3.260535</td>\n",
       "      <td>26.773197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>2024-06-26</td>\n",
       "      <td>60.0</td>\n",
       "      <td>55.329563</td>\n",
       "      <td>26.135740</td>\n",
       "      <td>3.284805</td>\n",
       "      <td>25.909016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>2024-06-27</td>\n",
       "      <td>51.0</td>\n",
       "      <td>54.512367</td>\n",
       "      <td>26.138510</td>\n",
       "      <td>3.308887</td>\n",
       "      <td>25.064970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>2024-06-28</td>\n",
       "      <td>69.0</td>\n",
       "      <td>57.033112</td>\n",
       "      <td>26.141281</td>\n",
       "      <td>3.332365</td>\n",
       "      <td>27.559464</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>130 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            ds     y      yhat1      trend  season_yearly  season_weekly\n",
       "0   2024-01-01   0.0  48.790421  25.645348      -3.249312      26.394384\n",
       "1   2024-01-02  46.0  49.413662  25.648117      -3.039501      26.805048\n",
       "2   2024-01-03  52.0  48.717308  25.650887      -2.821206      25.887630\n",
       "3   2024-01-04  70.0  48.204041  25.653662      -2.595083      25.145466\n",
       "4   2024-01-05  41.0  50.895260  25.656431      -2.363155      27.601984\n",
       "..         ...   ...        ...        ...            ...            ...\n",
       "125 2024-06-24  65.0  55.712456  26.130196       3.236652      26.345610\n",
       "126 2024-06-25  88.0  56.166698  26.132971       3.260535      26.773197\n",
       "127 2024-06-26  60.0  55.329563  26.135740       3.284805      25.909016\n",
       "128 2024-06-27  51.0  54.512367  26.138510       3.308887      25.064970\n",
       "129 2024-06-28  69.0  57.033112  26.141281       3.332365      27.559464\n",
       "\n",
       "[130 rows x 6 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # mod_blockrnn.fit(train['unidades'])\n",
    "# # mod_blockrnn.save('models/blockrnn')\n",
    "# mod_blockrnn = BlockRNNModel.load('models/blockrnn')\n",
    "\n",
    "# # mod_blocklstm.fit(train['unidades'])\n",
    "# # mod_blocklstm.save('models/blocklstm')\n",
    "# mod_blocklstm = BlockRNNModel.load('models/blocklstm')\n",
    "\n",
    "# # mod_blockgru.fit(train['unidades'])\n",
    "# # mod_blockgru.save('models/blockgru')\n",
    "# mod_blockgru = BlockRNNModel.load('models/blockgru')\n",
    "\n",
    "# mod_prophet.fit(train['unidades'])\n",
    "# mod_prophet.save('models/prophet')\n",
    "# mod_prophet = Prophet.load('models/prophet')\n",
    "\n",
    "# # mod_neuralprophet.fit(train_df[['fecha','unidades']].rename({'fecha':'ds', 'unidades':'y'}, axis=1))\n",
    "# # utils.save(mod_neuralprophet, 'models/neuralprophet')\n",
    "# mod_neuralprophet = utils.load('models/neuralprophet')\n",
    "\n",
    "# # mod_nbeats.fit(train['unidades'])\n",
    "# # mod_nbeats.save('models/nbeats')\n",
    "# mod_nbeats = NBEATSModel.load('models/nbeats')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento o carga de modelos multivariable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mod_blockrnn_multi.fit(train)\n",
    "# mod_blockrnn_multi.save('models/blockrnn_multi')\n",
    "mod_blockrnn_multi = BlockRNNModel.load('models/blockrnn_multi')\n",
    "\n",
    "# mod_blocklstm_multi.fit(train)\n",
    "# mod_blocklstm_multi.save('models/blocklstm_multi')\n",
    "mod_blocklstm_multi = BlockRNNModel.load('models/blocklstm_multi')\n",
    "\n",
    "# mod_blockgru_multi.fit(train)\n",
    "# mod_blockgru_multi.save('models/blockgru_multi')\n",
    "mod_blockgru_multi = BlockRNNModel.load('models/blockgru_multi')\n",
    "\n",
    "# mod_nbeats_multi.fit(train)\n",
    "# mod_nbeats_multi.save('models/nbeats_multi')\n",
    "mod_nbeats_multi = NBEATSModel.load('models/nbeats_multi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eeadb0fce4f4b98ba813e7f56ff0423",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0163c3ca54941cb8bd6e3672ad1546e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRU\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57d8d71532334bc58c9547652cf5bced",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prophet\n",
      "N-Beats\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7342c1643384589ba04abb3c0e0ebd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN-M\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fc68064eca34a0c82bb4307a0bdc856",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM-M\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "703e26b3fced4182a232ab210b22b087",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRU-M\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4985316b2ff44b03a4b68885300c2ca4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-Beats-M\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cedda76c52814139bba82b446fd84c42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - (NP.df_utils._infer_frequency) - Major frequency B corresponds to 99.231% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - B\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency B corresponds to 99.231% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - B\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44e808798be542e79a6b79412e46f0bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - (NP.df_utils.return_df_in_original_format) - Returning df with no ID column\n"
     ]
    }
   ],
   "source": [
    "models = [mod_blockrnn, mod_blocklstm, mod_blockgru, mod_prophet, mod_nbeats, mod_blockrnn_multi, mod_blocklstm_multi, mod_blockgru_multi, mod_nbeats_multi]\n",
    "names = ['RNN', 'LSTM', 'GRU', 'Prophet', 'N-Beats', 'RNN-M', 'LSTM-M', 'GRU-M', 'N-Beats-M']\n",
    "\n",
    "#test['unidades'].plot(label='mediciones')\n",
    "resultados = test['unidades'].pd_dataframe()\n",
    "errores = pd.DataFrame()\n",
    "\n",
    "# Se realizan las predicciones y calcula el error RMSE\n",
    "for i, model in enumerate(models):\n",
    "    print(names[i])\n",
    "    pred = model.predict(n=130)\n",
    "    predicion = pred['unidades'].pd_dataframe().rename({'unidades':names[i]}, axis=1)\n",
    "    resultados = pd.concat([resultados, predicion], axis=1)\n",
    "    err = rmse(pred['unidades'], test['unidades'])\n",
    "    errores = pd.concat([errores, pd.DataFrame({names[i]:[err]}, index=['Diario_escalado'])],axis=1)\n",
    "    #pred['unidades'].plot(label=names[i])\n",
    "\n",
    "# Se desescala las predicciones y calcula el error RMSE desescalado\n",
    "errores_aux = pd.DataFrame()\n",
    "for columna in resultados.columns:\n",
    "    resultados[columna] = scaler.inverse_transform(resultados[[columna]])\n",
    "    if columna == 'unidades':\n",
    "        continue\n",
    "    err=rootmse(resultados['unidades'], resultados[columna])\n",
    "    errores_aux = pd.concat([errores_aux, pd.DataFrame({columna:[err]}, index=['Diario'])],axis=1)\n",
    "errores = pd.concat([errores, errores_aux])\n",
    "\n",
    "# Predicciones para NeuralProphet\n",
    "predicion = mod_neuralprophet.predict(test_df[['fecha','unidades']].rename({'fecha':'ds', 'unidades':'y'}, axis=1))\n",
    "predicion = predicion.rename({'ds':'fecha', 'yhat1':'NeuralProphet'}, axis=1)\n",
    "predicion['fecha'] = pd.to_datetime(predicion['fecha'], format=\"%Y%m%d\")\n",
    "predicion = predicion.set_index(['fecha'])\n",
    "predicion = predicion[['NeuralProphet']]\n",
    "resultados = pd.concat([resultados, predicion], axis=1)\n",
    "err=rootmse(resultados['unidades'], resultados[columna])\n",
    "errores = pd.concat([errores, pd.DataFrame({'NeuralProphet':[err]}, index=['Diario'])],axis=1)\n",
    "\n",
    "#Agrupación por mes\n",
    "resultados_mes = resultados.groupby(pd.Grouper(freq='M'))\n",
    "resultados_mes = resultados_mes.sum()\n",
    "\n",
    "# Se calcula el error RMSE con los datos agrupados por meses\n",
    "errores_aux = pd.DataFrame()\n",
    "for columna in resultados_mes.columns:\n",
    "    if columna == 'unidades':\n",
    "        continue\n",
    "    err=rootmse(resultados_mes['unidades'], resultados_mes[columna])\n",
    "    errores_aux = pd.concat([errores_aux, pd.DataFrame({columna:[err]}, index=['Mensual'])],axis=1)\n",
    "errores = pd.concat([errores, errores_aux])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados.to_csv('results/resultados.csv')\n",
    "errores.fillna(0, inplace=True)\n",
    "errores.to_csv('results/errores.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zona de prueba de modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 101\u001b[0m\n\u001b[1;32m     87\u001b[0m mod_blockrnn \u001b[38;5;241m=\u001b[39m BlockRNNModel(\n\u001b[1;32m     88\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRNN\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     89\u001b[0m     input_chunk_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m260\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     98\u001b[0m     pl_trainer_kwargs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecision\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m64\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccelerator\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m     99\u001b[0m )\n\u001b[1;32m    100\u001b[0m mod_blockrnn_multi \u001b[38;5;241m=\u001b[39m cp(mod_blockrnn)\n\u001b[0;32m--> 101\u001b[0m \u001b[43mmod_blockrnn_multi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_encoders\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m mod_blocklstm \u001b[38;5;241m=\u001b[39m BlockRNNModel(\n\u001b[1;32m    104\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLSTM\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    105\u001b[0m     input_chunk_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m260\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    114\u001b[0m     pl_trainer_kwargs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecision\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m64\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccelerator\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m    115\u001b[0m )\n\u001b[1;32m    116\u001b[0m mod_blocklstm_multi \u001b[38;5;241m=\u001b[39m cp(mod_blocklstm)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "from copy import copy as cp\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from darts import TimeSeries\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "from darts.models import Prophet, BlockRNNModel, NBEATSModel, NHiTSModel, TCNModel, DLinearModel, NLinearModel, TiDEModel, TSMixerModel\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def tratamiento(fichero):\n",
    "    dataframe = pd.read_csv(fichero)\n",
    "    dataframe = dataframe.rename({'clave1':'fecha', 'uniTotal':'unidades'}, axis=1)\n",
    "    dataframe['fecha'] = pd.to_datetime(dataframe['fecha'], format=\"%Y%m%d\")\n",
    "    dataframe = dataframe.set_index(['fecha'])\n",
    "    dataframe = dataframe.resample('D').first()\n",
    "    dataframe.fillna(value=0, inplace=True)\n",
    "    dataframe = dataframe.groupby(pd.Grouper(freq='B'))\n",
    "    dataframe = dataframe.sum()\n",
    "    dataframe[dataframe['unidades'] < 0] = 0\n",
    "    dataframe = dataframe[\"2012-01-01\":\"2024-06-30\"]\n",
    "    name = fichero[fichero.find('/')+1:fichero.find('.')]\n",
    "    return name, dataframe\n",
    "\n",
    "def agrupar(ficheros: list):\n",
    "    df = pd.DataFrame()\n",
    "    lista = []\n",
    "    for archivo in ficheros:\n",
    "        name, aux = tratamiento(archivo)\n",
    "        aux = aux.rename({'unidades':name}, axis=1)\n",
    "        df = pd.concat([df, aux], axis=1)\n",
    "        lista.append(name)\n",
    "    return df, lista\n",
    "\n",
    "def grafico(dataframe):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(dataframe)\n",
    "    ax.minorticks_on()\n",
    "    ax.grid(which='major', alpha = 0.65, linestyle='-')\n",
    "    ax.grid(which='minor', alpha = 0.25, linestyle='--')\n",
    "    fig.autofmt_xdate()\n",
    "\n",
    "df, names = agrupar(['data/filtros.csv', 'data/baterias.csv', 'data/aceites.csv', 'data/limpiaparabrisas.csv'])\n",
    "\n",
    "# Meteorologia\n",
    "df2 = pd.read_csv('data/meteo_olvera.csv', decimal=',')\n",
    "df2['fecha'] = pd.to_datetime(df2['fecha'], format=\"%Y-%m-%d\")\n",
    "df2.set_index('fecha', inplace=True)\n",
    "df2 = df2[['tmed', 'prec', 'hrMedia']]\n",
    "df2 = df2.resample('B').first()\n",
    "df2 = df2.fillna(method='backfill')\n",
    "df2 = df2[\"2012-01-01\":\"2023-12-31\"]\n",
    "df = pd.concat([df, df2], axis=1)\n",
    "\n",
    "# Precio combustible\n",
    "df2 = pd.read_csv('data/carburante.csv', decimal=',')\n",
    "df2['fecha'] = pd.to_datetime(df2['fecha']+'0', format=\"%Y-%W%w\")\n",
    "df2 = df2.set_index('fecha').sort_index()\n",
    "df2 = df2.resample('B').first().ffill()\n",
    "df2 = df2[\"2012-01-01\":\"2023-12-31\"]\n",
    "\n",
    "df = pd.concat([df, df2], axis=1)\n",
    "\n",
    "df = df.reset_index()\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Definicion TimeSeries\n",
    "cols = ['tmed','prec', 'hrMedia', 'gasolina', 'diesel']\n",
    "\n",
    "series = TimeSeries.from_dataframe(df, time_col='fecha', value_cols=names+cols)\n",
    "\n",
    "series = series.add_holidays(country_code='ES', prov='AN')\n",
    "\n",
    "transformer = Scaler(scaler)\n",
    "series = transformer.fit_transform(series)\n",
    "\n",
    "train, test = series.split_after(pd.Timestamp(year=2023, month=12, day=31))\n",
    "\n",
    "# Definicion de modelos\n",
    "EPOCHS = 200\n",
    "BATCH = 256\n",
    "\n",
    "encoders = {'cyclic': {'future': ['month','year']}}\n",
    "\n",
    "mod_blockrnn = BlockRNNModel(\n",
    "    model='RNN',\n",
    "    input_chunk_length=260,\n",
    "    output_chunk_length=130,\n",
    "    hidden_dim=25,\n",
    "    n_rnn_layers=2,\n",
    "    dropout=0.2,\n",
    "    n_epochs=EPOCHS,\n",
    "    batch_size=BATCH,\n",
    "    show_warnings=True,\n",
    "    model_name='rnn',\n",
    "    pl_trainer_kwargs={\"precision\": '64', \"accelerator\": \"cpu\"}\n",
    ")\n",
    "mod_blockrnn_multi = cp(mod_blockrnn)\n",
    "mod_blockrnn_multi.add_encoders(encoders)\n",
    "\n",
    "mod_blocklstm = BlockRNNModel(\n",
    "    model='LSTM',\n",
    "    input_chunk_length=260,\n",
    "    output_chunk_length=130,\n",
    "    hidden_dim=25,\n",
    "    n_rnn_layers=2,\n",
    "    dropout=0.2,\n",
    "    n_epochs=EPOCHS,\n",
    "    batch_size=BATCH,\n",
    "    show_warnings=True,\n",
    "    model_name='lstm',\n",
    "    pl_trainer_kwargs={\"precision\": '64', \"accelerator\": \"cpu\"}\n",
    ")\n",
    "mod_blocklstm_multi = cp(mod_blocklstm)\n",
    "mod_blocklstm_multi.add_encoders(encoders)\n",
    "\n",
    "mod_blockgru = BlockRNNModel(\n",
    "    model='GRU',\n",
    "    input_chunk_length=260,\n",
    "    output_chunk_length=130,\n",
    "    hidden_dim=25,\n",
    "    n_rnn_layers=2,\n",
    "    dropout=0.2,\n",
    "    n_epochs=EPOCHS,\n",
    "    batch_size=BATCH,\n",
    "    show_warnings=True,\n",
    "    model_name='gru',\n",
    "    pl_trainer_kwargs={\"precision\": '64', \"accelerator\": \"cpu\"}\n",
    ")\n",
    "mod_blockgru_multi = cp(mod_blockgru)\n",
    "mod_blockgru_multi.add_encoders(encoders)\n",
    "\n",
    "mod_prophet = Prophet(\n",
    "    add_seasonalities=None, \n",
    "    country_holidays='ES', \n",
    "    suppress_stdout_stderror=True, \n",
    "    add_encoders=None, \n",
    "    cap=None, \n",
    "    floor=None\n",
    ")\n",
    "\n",
    "mod_nbeats = NBEATSModel(\n",
    "    input_chunk_length=260,\n",
    "    output_chunk_length=130,\n",
    "    dropout=0.2,\n",
    "    n_epochs=EPOCHS,\n",
    "    show_warnings=True,\n",
    "    batch_size=BATCH,\n",
    "    model_name='nbeats',\n",
    "    pl_trainer_kwargs={\"precision\": '64', \"accelerator\": \"gpu\", \"devices\": -1, \"auto_select_gpus\": True}\n",
    ")\n",
    "mod_nbeats_multi = cp(mod_nbeats)\n",
    "mod_nbeats_multi.add_encoders(encoders)\n",
    "\n",
    "mod_nhits = NHiTSModel(\n",
    "    input_chunk_length=260,\n",
    "    output_chunk_length=130,\n",
    "    dropout=0.2,\n",
    "    n_epochs=EPOCHS,\n",
    "    show_warnings=True,\n",
    "    batch_size=BATCH,\n",
    "    model_name='nhits',\n",
    "    pl_trainer_kwargs={\"precision\": '64', \"accelerator\": \"gpu\", \"devices\": -1, \"auto_select_gpus\": True}\n",
    ")\n",
    "mod_nhits_multi = cp(mod_nhits)\n",
    "mod_nhits_multi.add_encoders(encoders)\n",
    "\n",
    "mod_tcn = TCNModel(\n",
    "    input_chunk_length=260,\n",
    "    output_chunk_length=130,\n",
    "    dropout=0.2,\n",
    "    n_epochs=EPOCHS,\n",
    "    show_warnings=True,\n",
    "    batch_size=BATCH,\n",
    "    model_name='tcn',\n",
    "    pl_trainer_kwargs={\"precision\": '64', \"accelerator\": \"gpu\", \"devices\": -1, \"auto_select_gpus\": True}\n",
    ")\n",
    "mod_tcn_multi = cp(mod_tcn)\n",
    "mod_tcn_multi.add_encoders(encoders)\n",
    "\n",
    "mod_dlinear = DLinearModel(\n",
    "    input_chunk_length=260,\n",
    "    output_chunk_length=130,\n",
    "    n_epochs=EPOCHS,\n",
    "    show_warnings=True,\n",
    "    batch_size=BATCH,\n",
    "    model_name='dlinear',\n",
    "    pl_trainer_kwargs={\"precision\": '64', \"accelerator\": \"gpu\", \"devices\": -1, \"auto_select_gpus\": True}\n",
    ")\n",
    "mod_dlinear_multi = cp(mod_dlinear)\n",
    "mod_dlinear_multi.add_encoders(encoders)\n",
    "\n",
    "mod_nlinear = NLinearModel(\n",
    "    input_chunk_length=260,\n",
    "    output_chunk_length=130,\n",
    "    n_epochs=EPOCHS,\n",
    "    show_warnings=True,\n",
    "    batch_size=BATCH,\n",
    "    model_name='nlinear',\n",
    "    pl_trainer_kwargs={\"precision\": '64', \"accelerator\": \"gpu\", \"devices\": -1, \"auto_select_gpus\": True}\n",
    ")\n",
    "mod_nlinear_multi = cp(mod_nlinear)\n",
    "mod_nlinear_multi.add_encoders(encoders)\n",
    "\n",
    "mod_tide = TiDEModel(\n",
    "    input_chunk_length=260,\n",
    "    output_chunk_length=130,\n",
    "    dropout=0.2,\n",
    "    n_epochs=EPOCHS,\n",
    "    show_warnings=True,\n",
    "    batch_size=BATCH,\n",
    "    model_name='tide',\n",
    "    pl_trainer_kwargs={\"precision\": '64', \"accelerator\": \"gpu\", \"devices\": -1, \"auto_select_gpus\": True}\n",
    ")\n",
    "mod_tide_multi = cp(mod_tide)\n",
    "mod_tide_multi.add_encoders(encoders)\n",
    "\n",
    "mod_tsmixer = TSMixerModel(\n",
    "    input_chunk_length=260,\n",
    "    output_chunk_length=130,\n",
    "    dropout=0.2,\n",
    "    n_epochs=EPOCHS,\n",
    "    show_warnings=True,\n",
    "    batch_size=BATCH,\n",
    "    model_name='tsmixer',\n",
    "    pl_trainer_kwargs={\"precision\": '64', \"accelerator\": \"gpu\", \"devices\": -1, \"auto_select_gpus\": True}\n",
    ")\n",
    "mod_tsmixer_multi = cp(mod_tsmixer)\n",
    "mod_tsmixer_multi.add_encoders(encoders)\n",
    "\n",
    "def reset_models():\n",
    "    mod_blockrnn.reset_model()\n",
    "    mod_blocklstm.reset_model()\n",
    "    mod_blockgru.reset_model()\n",
    "    mod_nbeats.reset_model()\n",
    "    mod_nhits.reset_model()\n",
    "    mod_tcn.reset_model()\n",
    "    mod_dlinear.reset_model()\n",
    "    mod_nlinear.reset_model()\n",
    "    mod_tide.reset_model()\n",
    "    mod_tsmixer.reset_model()\n",
    "\n",
    "    mod_blockrnn_multi.reset_model()\n",
    "    mod_blocklstm_multi.reset_model()\n",
    "    mod_blockgru_multi.reset_model()\n",
    "    mod_nbeats_multi.reset_model()\n",
    "    mod_nhits_multi.reset_model()\n",
    "    mod_tcn_multi.reset_model()\n",
    "    mod_dlinear_multi.reset_model()\n",
    "    mod_nlinear_multi.reset_model()\n",
    "    mod_tide_multi.reset_model()\n",
    "    mod_tsmixer_multi.reset_model()\n",
    "\n",
    "# Entrenamiento\n",
    "for name in names:\n",
    "    reset_models()\n",
    "    print(name)\n",
    "\n",
    "    # # Modelos sin covariates\n",
    "    # mod_blockrnn.fit(train[f'{name}'])\n",
    "    # mod_blockrnn.save(f'models/{name}/blockrnn')\n",
    "\n",
    "    # mod_blocklstm.fit(train[f'{name}'])\n",
    "    # mod_blocklstm.save(f'models/{name}/blocklstm')\n",
    "\n",
    "    # mod_blockgru.fit(train[f'{name}'])\n",
    "    # mod_blockgru.save(f'models/{name}/blockgru')\n",
    "\n",
    "    # mod_prophet.fit(train[f'{name}'])\n",
    "    # mod_prophet.save(f'models/{name}/prophet')\n",
    "\n",
    "    # mod_nbeats.fit(train[f'{name}'])\n",
    "    # mod_nbeats.save(f'models/{name}/nbeats')\n",
    "\n",
    "    # mod_nhits.fit(train[f'{name}'])\n",
    "    # mod_nhits.save(f'models/{name}/nhits')\n",
    "\n",
    "    # mod_tcn.fit(train[f'{name}'])\n",
    "    # mod_tcn.save(f'models/{name}/tcn')\n",
    "\n",
    "    # mod_dlinear.fit(train[f'{name}'])\n",
    "    # mod_dlinear.save(f'models/{name}/dlinear')\n",
    "\n",
    "    # mod_nlinear.fit(train[f'{name}'])\n",
    "    # mod_nlinear.save(f'models/{name}/nlinear')\n",
    "\n",
    "    # mod_tide.fit(train[f'{name}'])\n",
    "    # mod_tide.save(f'models/{name}/tide')\n",
    "\n",
    "    # mod_tsmixer.fit(train[f'{name}'])\n",
    "    # mod_tsmixer.save(f'models/{name}/tsmixer')\n",
    "\n",
    "    # # Modelos con covariates\n",
    "    # mod_blockrnn_multi.fit(series=train[name], past_covariates=train.drop_columns(names))\n",
    "    # mod_blockrnn_multi.save(f'models/{name}/blockrnn_multi')\n",
    "\n",
    "    # mod_blocklstm_multi.fit(series=train[name], past_covariates=train.drop_columns(names))\n",
    "    # mod_blocklstm_multi.save(f'models/{name}/blocklstm_multi')\n",
    "\n",
    "    # mod_blockgru_multi.fit(series=train[name], past_covariates=train.drop_columns(names))\n",
    "    # mod_blockgru_multi.save(f'models/{name}/blockgru_multi')\n",
    "\n",
    "    # mod_nbeats_multi.fit(series=train[name], past_covariates=train.drop_columns(names))\n",
    "    # mod_nbeats_multi.save(f'models/{name}/nbeats_multi')\n",
    "\n",
    "    # mod_nhits_multi.fit(series=train[name], past_covariates=train.drop_columns(names))\n",
    "    # mod_nhits_multi.save(f'models/{name}/nhits_multi')\n",
    "\n",
    "    # mod_tcn_multi.fit(series=train[name], past_covariates=train.drop_columns(names))\n",
    "    # mod_tcn_multi.save(f'models/{name}/tcn_multi')\n",
    "\n",
    "    # mod_dlinear_multi.fit(series=train[name], past_covariates=train[['tmed', 'prec', 'hrMedia', 'gasolina', 'diesel']], future_covariates=series['holidays'])\n",
    "    # mod_dlinear_multi.save(f'models/{name}/dlinear_multi')\n",
    "\n",
    "    # mod_nlinear_multi.fit(series=train[name], past_covariates=train[['tmed', 'prec', 'hrMedia', 'gasolina', 'diesel']], future_covariates=series['holidays'])\n",
    "    # mod_nlinear_multi.save(f'models/{name}/nlinear_multi')\n",
    "\n",
    "    # mod_tide_multi.fit(series=train[name], past_covariates=train[['tmed', 'prec', 'hrMedia', 'gasolina', 'diesel']], future_covariates=series['holidays'])\n",
    "    # mod_tide_multi.save(f'models/{name}/tide_multi')\n",
    "    \n",
    "    # mod_tsmixer_multi.fit(series=train[name], past_covariates=train[['tmed', 'prec', 'hrMedia', 'gasolina', 'diesel']], future_covariates=series['holidays'])\n",
    "    # mod_tsmixer_multi.save(f'models/{name}/tsmixer_multi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_blockrnn = BlockRNNModel(\n",
    "    model='RNN',\n",
    "    input_chunk_length=260,\n",
    "    output_chunk_length=130,\n",
    "    hidden_dim=25,\n",
    "    n_rnn_layers=2,\n",
    "    dropout=0.2,\n",
    "    n_epochs=EPOCHS,\n",
    "    batch_size=BATCH,\n",
    "    show_warnings=True,\n",
    "    model_name='rnn',\n",
    "    pl_trainer_kwargs={\"precision\": '64', \"accelerator\": \"cpu\"},\n",
    "    add_encoders=encoders\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
